{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07d57059-2905-4e90-ad77-42fe0b1bb4e0",
   "metadata": {},
   "source": [
    "# 3. Predictive Text, Part I: N-Grams\n",
    "\n",
    "In this section, we will be building predictive text, a system that suggests the next word while a user is typing. Predictive text is used in mobile phone keyboards, search applications, AI-powered email composition, and more.\n",
    "\n",
    "The primary goal of a predictive text system is **given some sequence of words, what is the most likely next word?**\n",
    "\n",
    "This is essentially the same problem as the NLP concept of a **Language Model**. A language model attempts to calculate the probability of a sequence of words. Creating good language models is a huge field of NLP, and many of the state-of-the-art language models today require enormous amounts of data. However, we typically do not have sufficient data for low-resource languages to use these sorts of models, so this tutorial will use models that are effective even with more limited corpora.\n",
    "\n",
    "## Loading the corpus\n",
    "For this application, we are using english text from the COCA corpus. We chose to use English rather than Uspanteko so that the reader can easily judge the goodness of the predictions. However, feel free to use your own preferred language, making the necessary changes when indicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a229d2-5414-4547-bcfd-32f23c822fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n@@17141 ERIC @!BURNS , FOX NEWS HOST : On this week \\'s \" FOX News Watch \" : The president and the hurricane : will he weather the storm ? The media helping victims to weather the storm . How did the media judge the judge ? How did audiences judge Martha on her return to TV ? And everybody has to go to the bathroom sometime . We will cover the coverage of these stories right after the headlines . @(NEWSBREAK) @!BURNS : You \\'ve heard of slow times in the news business ? The past few weeks have been as fast as they get . So we had better get started . Here are Jim Pinkerton of \" Newsday \" , syndicated columnist Cal Thomas , Jane Hall of the American University , and media writer Neal Gabler . I \\'m Eric Burns . \" FOX News Watch \" is coming right up . @(BEGIN-VIDEO-CLIP) @!GEORGE-W-BUSH-PRE : Americans have every right to expect a more effective response in a time of emergency . When the federal government fails to meet such an obligation , I as president am responsible for the problem . A'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our data\n",
    "import util\n",
    "corpus = util.load_raw_text(corpus_directory=\"corpus-eng\") # REPLACE WITH YOUR CORPUS FOLDER\n",
    "corpus[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09635b7-7801-497b-8bdd-5e8929a26c21",
   "metadata": {},
   "source": [
    "Next, we need to tokenize our text. However, we want to keep punctuation that ends a sentence this time, since we care about dividing the text into sentences.\n",
    "\n",
    "If you are using your own language, you will need to split the data into a list of words, including ending punctuation. You can do this with a tokenizing function based on the format of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e5d13b-434c-4a64-b53c-2ce55435306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1041158\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['eric',\n",
       " ',',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'host',\n",
       " 'on',\n",
       " 'this',\n",
       " 'week',\n",
       " \"'s\",\n",
       " 'fox',\n",
       " 'news',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'president',\n",
       " 'and',\n",
       " 'the',\n",
       " 'hurricane',\n",
       " 'will',\n",
       " 'he',\n",
       " 'weather',\n",
       " 'the',\n",
       " 'storm',\n",
       " '?',\n",
       " 'the',\n",
       " 'media',\n",
       " 'helping',\n",
       " 'victims',\n",
       " 'to',\n",
       " 'weather',\n",
       " 'the',\n",
       " 'storm',\n",
       " '.',\n",
       " 'how',\n",
       " 'did',\n",
       " 'the',\n",
       " 'media',\n",
       " 'judge',\n",
       " 'the',\n",
       " 'judge',\n",
       " '?',\n",
       " 'how',\n",
       " 'did',\n",
       " 'audiences',\n",
       " 'judge',\n",
       " 'martha',\n",
       " 'on',\n",
       " 'her',\n",
       " 'return',\n",
       " 'to',\n",
       " 'tv',\n",
       " '?',\n",
       " 'and',\n",
       " 'everybody',\n",
       " 'has',\n",
       " 'to',\n",
       " 'go',\n",
       " 'to',\n",
       " 'the',\n",
       " 'bathroom',\n",
       " 'sometime',\n",
       " '.',\n",
       " 'we',\n",
       " 'will',\n",
       " 'cover',\n",
       " 'the',\n",
       " 'coverage',\n",
       " 'of',\n",
       " 'these',\n",
       " 'stories',\n",
       " 'right',\n",
       " 'after',\n",
       " 'the',\n",
       " 'headlines',\n",
       " '.',\n",
       " 'you',\n",
       " \"'ve\",\n",
       " 'heard',\n",
       " 'of',\n",
       " 'slow',\n",
       " 'times',\n",
       " 'in',\n",
       " 'the',\n",
       " 'news',\n",
       " 'business',\n",
       " '?',\n",
       " 'the',\n",
       " 'past',\n",
       " 'few',\n",
       " 'weeks',\n",
       " 'have',\n",
       " 'been',\n",
       " 'as',\n",
       " 'fast',\n",
       " 'as',\n",
       " 'they',\n",
       " 'get',\n",
       " '.',\n",
       " 'so',\n",
       " 'we',\n",
       " 'had']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess(text):\n",
    "    text = util.strip_accents(text)\n",
    "    text = text.lower()\n",
    "\n",
    "    # Unlike our Uspanteko data, this is already separated by spaces, so we can just split by spaces (rather than tokenizing)\n",
    "    # REPLACE WITH TOKENIZATION METHOD FOR YOUR LANGUAGE\n",
    "    tokens = text.split(\" \")\n",
    "\n",
    "    tokens_filtered = []\n",
    "    for token in tokens:\n",
    "        # Skip any tokens with special characters\n",
    "        if re.match(r\"[\\w|\\']+|[\\.|\\,|\\?|\\!]\", token):\n",
    "            tokens_filtered.append(token)\n",
    "    return tokens_filtered\n",
    "\n",
    "\n",
    "tokens_filtered = preprocess(corpus)\n",
    "print(len(tokens_filtered))\n",
    "tokens_filtered[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebfcc5-750e-44dd-bec1-39697b9a13c7",
   "metadata": {},
   "source": [
    "This corpus is much larger than most low-resource language corpora will be, so we will shorten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340cbcbd-d8a4-4ceb-8a7b-7895628c3b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_filtered = tokens_filtered[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc33e519-8ad6-4c70-ae07-3504f9ca542e",
   "metadata": {},
   "source": [
    "## N-Grams\n",
    "Now, let's build our first model. For this we will use *n-grams*.\n",
    "\n",
    "An n-gram is simply a sequence of *n* words that occurs in our text. For instance, consider the sentence:\n",
    "\n",
    "> What is your name?\n",
    "\n",
    "If we got the list of all of the *bigrams* (2 words each) in the sentence, we would have:\n",
    "\n",
    "- *What is*\n",
    "- *is your*\n",
    "- *your name*\n",
    "- *name ?*\n",
    "\n",
    "If we got the list of all of the *trigrams* (3 words each) we would get:\n",
    "\n",
    "- *What is your*\n",
    "- *is your name*\n",
    "- *your name ?*\n",
    "\n",
    "And so on, and so forth.\n",
    "\n",
    "### Using N-grams as a language model\n",
    "\n",
    "We can easily use the idea of n-grams to build a simple language model. For instance, if we are trying to predict the next word, using trigrams, given the input:\n",
    "\n",
    "> What is your ...\n",
    "\n",
    "We can look at all of the trigrams that start with *is your*, and choose the most common one. Let's use our tokenized text to get a list of all of the n-grams that occur in the text. \n",
    "\n",
    "**Note:** Right now, we would have n-grams that cross sentence boundaries, such as \"headlines . you\". This isn't super useful, so a common technique is to **pad** each sentence with tokens representing the start of the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75ecaa5-650e-4a11-8e1a-eafca5a9681a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " '<s>',\n",
       " 'eric',\n",
       " ',',\n",
       " 'fox',\n",
       " 'news',\n",
       " 'host',\n",
       " 'on',\n",
       " 'this',\n",
       " 'week',\n",
       " \"'s\",\n",
       " 'fox',\n",
       " 'news',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'president',\n",
       " 'and',\n",
       " 'the',\n",
       " 'hurricane',\n",
       " 'will',\n",
       " 'he',\n",
       " 'weather',\n",
       " 'the',\n",
       " 'storm',\n",
       " '?',\n",
       " '<s>',\n",
       " '<s>',\n",
       " 'the',\n",
       " 'media',\n",
       " 'helping']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad each sentence with some number of start symbols \n",
    "def pad(text: list, num_padding: int):\n",
    "    padded_text = []\n",
    "    \n",
    "    # Add initial padding to the first sentence\n",
    "    for _ in range(num_padding):\n",
    "        padded_text.append(\"<s>\")\n",
    "    \n",
    "    for word in text:\n",
    "        padded_text.append(word)\n",
    "\n",
    "        # Every time we see an end punctuation mark, add <s> tokens before it\n",
    "        # REPLACE IF YOUR LANGUAGE USES DIFFERENT END PUNCTUATION\n",
    "        if word in [\".\", \"?\", \"!\"]:\n",
    "            for _ in range(num_padding):\n",
    "                padded_text.append(\"<s>\")\n",
    "        \n",
    "        \n",
    "    return padded_text\n",
    "\n",
    "pad(tokens_filtered, 2)[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90f9feb2-b1bf-4da2-a6ae-fe001fd7bdd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<s>', '<s>', 'eric'),\n",
       " ('<s>', 'eric', ','),\n",
       " ('eric', ',', 'fox'),\n",
       " (',', 'fox', 'news'),\n",
       " ('fox', 'news', 'host'),\n",
       " ('news', 'host', 'on'),\n",
       " ('host', 'on', 'this'),\n",
       " ('on', 'this', 'week'),\n",
       " ('this', 'week', \"'s\"),\n",
       " ('week', \"'s\", 'fox'),\n",
       " (\"'s\", 'fox', 'news'),\n",
       " ('fox', 'news', 'watch'),\n",
       " ('news', 'watch', 'the'),\n",
       " ('watch', 'the', 'president'),\n",
       " ('the', 'president', 'and'),\n",
       " ('president', 'and', 'the'),\n",
       " ('and', 'the', 'hurricane'),\n",
       " ('the', 'hurricane', 'will'),\n",
       " ('hurricane', 'will', 'he'),\n",
       " ('will', 'he', 'weather'),\n",
       " ('he', 'weather', 'the'),\n",
       " ('weather', 'the', 'storm'),\n",
       " ('the', 'storm', '?'),\n",
       " ('storm', '?', '<s>'),\n",
       " ('?', '<s>', '<s>'),\n",
       " ('<s>', '<s>', 'the'),\n",
       " ('<s>', 'the', 'media'),\n",
       " ('the', 'media', 'helping'),\n",
       " ('media', 'helping', 'victims'),\n",
       " ('helping', 'victims', 'to')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "# Now, we can actually create the list of n-grams using the NLTK library\n",
    "padded_tokens = pad(tokens_filtered, 2)\n",
    "trigrams = list(ngrams(sequence=padded_tokens, n=3))\n",
    "trigrams[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7450ed6-be5d-4565-a30a-e0d4703ec185",
   "metadata": {},
   "source": [
    "Now that we have a list of trigrams, we can count up the frequency of each different trigram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cccbdd25-9dca-42d1-ba39-60e24d8fad81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41845"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a dict of all trigrams and their frequency\n",
    "# This is the same as how we counted words in the spellchecker section\n",
    "all_trigrams = dict()\n",
    "for gram in trigrams:\n",
    "    if gram in all_trigrams:\n",
    "        all_trigrams[gram] += 1\n",
    "    else:\n",
    "        all_trigrams[gram] = 1\n",
    "        \n",
    "len(all_trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2801c8a4-668d-4800-9b64-8094525e5306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('.', '<s>', '<s>'), 2379),\n",
       " (('?', '<s>', '<s>'), 338),\n",
       " (('!', '<s>', '<s>'), 242),\n",
       " (('qwq', '!', '<s>'), 235),\n",
       " (('<s>', '<s>', 'qwq'), 202),\n",
       " (('<s>', 'qwq', '!'), 201),\n",
       " (('<s>', '<s>', 'and'), 172),\n",
       " (('<s>', '<s>', 'i'), 166),\n",
       " (('<s>', '<s>', 'but'), 109),\n",
       " (('<s>', '<s>', 'the'), 98),\n",
       " (('<s>', '<s>', 'it'), 96),\n",
       " (('<s>', '<s>', 'we'), 68),\n",
       " (('<s>', '<s>', 'he'), 62),\n",
       " (('you', 'know', ','), 61),\n",
       " (('<s>', '<s>', 'tom'), 60),\n",
       " (('<s>', 'tom', 'jarriel'), 60),\n",
       " (('<s>', '<s>', 'they'), 54),\n",
       " (('<s>', '<s>', 'so'), 52),\n",
       " (('<s>', '<s>', 'simon'), 50),\n",
       " (('it', '.', '<s>'), 50),\n",
       " (('<s>', '<s>', 'that'), 48),\n",
       " ((',', 'you', 'know'), 47),\n",
       " (('<s>', 'it', \"'s\"), 45),\n",
       " (('<s>', '<s>', 'you'), 44),\n",
       " (('<s>', '<s>', 'this'), 42),\n",
       " (('<s>', '<s>', 'in'), 42),\n",
       " (('tom', 'jarriel', 'voice-over'), 38),\n",
       " (('a', 'lot', 'of'), 36),\n",
       " (('<s>', '<s>', 'stewart'), 36),\n",
       " (('i', 'mean', ','), 36)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what the twenty most common trigrams are\n",
    "sorted(all_trigrams.items(), key=lambda x: x[1], reverse=True)[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c76b65-79df-4c99-b5b9-6ce38cdc1ab4",
   "metadata": {},
   "source": [
    "Now that we have a count of all trigrams, we can make predictions by looking for the most common trigram that matches our input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70eaa88a-c063-4911-a5b5-5191232a65b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start of a sentence, with every word and punctuation mark surrounded by spaces: Hi my name is\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:  ['ana']\n"
     ]
    }
   ],
   "source": [
    "def predict_trigram_model(number_results = 3):\n",
    "    text = input(\"Enter the start of a sentence, with every word and punctuation mark surrounded by spaces:\")\n",
    "    input_tokens = pad(preprocess(text), 2)\n",
    "    \n",
    "    # Find the last 2 tokens in the input\n",
    "    last_two_tokens = input_tokens[-2:]\n",
    "    \n",
    "    # Search our list of all trigrams to find matching trigrams\n",
    "    matching_trigrams = []\n",
    "    for item in all_trigrams.items():\n",
    "        gram = item[0]\n",
    "        \n",
    "        # Check if the first and second item in the trigram are a match\n",
    "        if gram[0] == last_two_tokens[0] and gram[1] == last_two_tokens[1]:\n",
    "            matching_trigrams.append(item)\n",
    "    \n",
    "    # Now, sort the matching trigrams by popularity and return the first `number_results` results\n",
    "    sorted_matching_trigrams = sorted(matching_trigrams, key=lambda x: x[1], reverse=True)\n",
    "    top_matching_trigrams = sorted_matching_trigrams[:number_results]\n",
    "    \n",
    "    # Last, let's just get the predicted word (the last word of the trigram)\n",
    "    predictions = [trigram[0][2] for trigram in top_matching_trigrams]\n",
    "    return predictions\n",
    "\n",
    "print(\"Predictions: \", predict_trigram_model())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb2d2e-42dd-4933-86d0-3915715064bf",
   "metadata": {},
   "source": [
    "### Considerations with n-grams\n",
    "This is great, but it has one significant issue. If the phrase we enter ends with two words that don't match *any* trigram, then our model has no predictions to make.\n",
    "\n",
    "One common solution for this is to use a process called **backoff**. With backoff, if our trigram model doesn't find any results, we try to find matching *bigrams* instead. If that fails, we go to *unigrams* (which means we just use the most frequent words). This means that we always get a result, even if it isn't quite as good.\n",
    "\n",
    "Below is a model that uses backoff and an arbitrary size n-gram to make predictions, putting together everything so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc969a71-88c0-480e-9e3c-c3cc5d6643f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the start of a sentence, with every word and punctuation mark surrounded by spaces: What are you\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['hearing']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_gram_models = dict()\n",
    "\n",
    "def create_ngram_model(n = 3):\n",
    "    padded_tokens = pad(tokens_filtered, n - 1)\n",
    "    grams = list(ngrams(sequence=padded_tokens, n=n))\n",
    "    \n",
    "    all_ngrams = dict()\n",
    "    for gram in grams:\n",
    "        if gram in all_ngrams:\n",
    "            all_ngrams[gram] += 1\n",
    "        else:\n",
    "            all_ngrams[gram] = 1\n",
    "    return all_ngrams\n",
    "\n",
    "def predict_ngram_model(n = 3, number_results = 3):\n",
    "    text = input(\"Enter the start of a sentence, with every word and punctuation mark surrounded by spaces:\")\n",
    "    input_tokens = pad(preprocess(text), n - 1)\n",
    "    \n",
    "    while n > 0:\n",
    "        # Find the last n - 1 tokens in the input\n",
    "        last_tokens = tuple(input_tokens[-(n-1):])\n",
    "    \n",
    "        if not n in n_gram_models:\n",
    "            n_gram_models[n] = create_ngram_model(n)\n",
    "        \n",
    "        matching_ngrams = []\n",
    "        \n",
    "        for item in n_gram_models[n].items():\n",
    "            gram = item[0]\n",
    "            if gram[:-1] == last_tokens:\n",
    "                matching_ngrams.append(item)\n",
    "    \n",
    "        # Now, sort the matching trigrams by popularity and return the first `number_results` results\n",
    "        sorted_matching_ngrams = sorted(matching_ngrams, key=lambda x: x[1], reverse=True)\n",
    "        top_matching_ngrams = sorted_matching_ngrams[:number_results]\n",
    "        \n",
    "        # BACKOFF: If there are no results, drop n and try again\n",
    "        if len(top_matching_ngrams) == 0:\n",
    "            print(\"backing off to:\", n-1)\n",
    "            n = n - 1\n",
    "            continue\n",
    "    \n",
    "        # Last, let's just get the predicted word (the last word of the trigram)\n",
    "        predictions = [gram[0][-1] for gram in top_matching_ngrams]\n",
    "        return predictions\n",
    "    return []\n",
    "\n",
    "predict_ngram_model(4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
