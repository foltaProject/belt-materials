{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66bf47f6-8bda-4d52-9180-9ddcbd44ccc2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "# **2. Texto Preditivo, Parte I: N-Gramas**\n",
        "\n",
        "<div>\n",
        "<img src=\"../assets/autocomplete.png\" width=\"500\" style=\" display: block; margin-left: auto; margin-right: auto;\"/>\n",
        "</div>\n",
        "\n",
        "Nessa seção, construiremos **texto preditivo**, um sistema que sugere a palavra seguinte enquanto um usuário digita. Texto preditivo é usado em teclados de celular, aplicativos de busca, composição por e-mail com força IA e muito mais.\n",
        "\n",
        "O objetivo principal de um sistema de texto preditivo é *dada uma sequência de palavras, prever a próxima palavra mais provável*.\n",
        "\n",
        "#### Como fazemos isso?\n",
        "Em Processamento de Linguagem Natural (PLN), isso pode ser resolvido com um **modelo de idioma**. Um modelo de idioma aprende a distribuição de palavras no texto. Vamos construir um modelo de linguagem simples que aprende frases comuns de duas ou três palavras.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6f4658-fc08-4102-9060-29bca34ce78d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Data Preparation**\n",
        "### Carregar o corpus\n",
        "Para esta aplicação, utilizamos texto em inglês do corpus COCA. \n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    Como antes, sinta-se à vontade para usar sua própria língua.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a229d2-5414-4547-bcfd-32f23c822fe1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load our data\n",
        "import util\n",
        "\n",
        "# REPLACE WITH YOUR CORPUS DIRECTORY\n",
        "corpus = util.load_raw_text(corpus_directory=\"../../corpora/eng\")\n",
        "corpus[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09635b7-7801-497b-8bdd-5e8929a26c21",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Pré-processamento e Tokenização\n",
        "Como no corretor automático, o próximo passo é de tokenizar o texto em palavras individuais. A única diferença aqui é que mantemos a pontuação.\n",
        "\n",
        "#### **Exercício 1**\n",
        "Use o regex fornecido para tokenizar o texto e retornar o resultado.\n",
        "\n",
        "detalhes\n",
        "  <summary>Mostrar resposta</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">return re.findall(word_or_punctuation_regex, text)</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e5d13b-434c-4a64-b53c-2ce55435306e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "word_or_punctuation_regex = r\"[\\w|\\']+|[\\.|\\,|\\?|\\!]\"\n",
        "\n",
        "def preprocess(text):\n",
        "    text = util.strip_accents(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # TODO: Use the provided regex to tokenize the text, and return the result\n",
        "\n",
        "tokens_filtered = preprocess(corpus)\n",
        "\n",
        "print(len(tokens_filtered), \"total tokens\")\n",
        "print(tokens_filtered[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6801ad-251b-466f-9ea2-e50fa58b22b2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **N-Gram Modelamento**\n",
        "Agora, vamos construir nosso primeiro modelo. Para isso, usaremos *n-gramas*.\n",
        "\n",
        "Um n-grama é simplesmente uma sequência de *n* palavras que ocorre em nosso texto. Por exemplo, considere a frase:\n",
        "\n",
        "Como é seu nome?\n",
        "\n",
        "Se tivéssemos a lista de todos os *bigramas* (2 palavras cada) na frase, teríamos:\n",
        "\n",
        "- *Como é*\n",
        "- *é seu*\n",
        "- *seu nome*\n",
        "- *nome ?*\n",
        "\n",
        "Se tivéssemos a lista de todos os *bigramas* (3 palavras cada) na frase, teríamos:\n",
        "\n",
        "- *Como é seu*\n",
        "- *é seu nome*\n",
        "- *seu nome* ?*\n",
        "\n",
        "E assim por diante."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49609b51-0373-45d2-bf28-d25a4ad8cb79",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Usando N-gramas como modelo de idioma\n",
        "\n",
        "Podemos facilmente usar a ideia de n-gramas para construir um modelo de idioma simples. Por exemplo, se estamos tentando prever a próxima palavra, usando trigramas, dado a entrada:\n",
        "\n",
        "> Como é seu* ...\n",
        "\n",
        "Podemos olhar todos os trigramas que começam com *é seu*, e escolher o mais comum. Vamos usar nosso texto tokenizado para obter uma lista de todos os n-gramas que aparecem no texto. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d3d04e-2ff1-45ac-9e76-8d97f69243ba",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Preenchimento de frases\n",
        "Neste momento, teríamos n-gramas que passariam os limites das frases, como \"manchetes de você\". Isso não é muito útil, então uma técnica comum é **preencher** cada frase com os tokens representando o início da frase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75ecaa5-650e-4a11-8e1a-eafca5a9681a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad(text: list, num_padding: int):\n",
        "    \n",
        "    padded_text = []\n",
        "    \n",
        "    # Add initial padding to the first sentence\n",
        "    for _ in range(num_padding):\n",
        "        padded_text.append(\"<s>\")\n",
        "    \n",
        "    for word in text:\n",
        "        padded_text.append(word)\n",
        "\n",
        "        # Every time we see an end punctuation mark, add <s> tokens after it\n",
        "        # REPLACE IF YOUR LANGUAGE USES DIFFERENT END PUNCTUATION\n",
        "        if word in [\".\", \"?\", \"!\"]:\n",
        "            for _ in range(num_padding):\n",
        "                padded_text.append(\"<s>\")\n",
        "        \n",
        "        \n",
        "    return padded_text\n",
        "\n",
        "print(pad(tokens_filtered, 2)[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add6e534-f8de-4b0d-bd82-f0637a8bf8c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Criar uma lista de n-gramas\n",
        "O código a seguir usa a biblioteca **NLTK** para criar uma lista de trigramas.\n",
        "\n",
        "#### **Exercício 2**\n",
        "Em vez disso, o que mudaríamos para utilizar o bigrama?\n",
        "\n",
        "<details>\n",
        "  <summary>Mostrar resposta</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">padded_tokens = pad(tokens_filtered, 1)\n",
        "trigrams = list(ngrams(sequence=padded_tokens, n=2))</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a031ba-3575-4080-917b-f38471fcb545",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "# Now, we can actually create the list of n-grams using the NLTK library\n",
        "padded_tokens = pad(tokens_filtered, 2)\n",
        "trigrams = list(ngrams(sequence=padded_tokens, n=3))\n",
        "trigrams[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7450ed6-be5d-4565-a30a-e0d4703ec185",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "Agora que temos uma lista de trigramas, podemos contar a frequência de cada trigrama diferente.\n",
        "\n",
        "#### **Exercício 3**\n",
        "Usando a lista de trigramas, preencha o dicionário `todos_trigrams`, tal que cada chave é um trigrama único e o valor é quantas vezes esse trigrama ocorre no texto.\n",
        "\n",
        "<details>\n",
        "  <summary>Mostrar resposta</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">for gram in trigrams:\n",
        "    if gram in all_trigrams:\n",
        "        all_trigrams[gram] += 1\n",
        "    else:\n",
        "        all_trigrams[gram] = 1</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cccbdd25-9dca-42d1-ba39-60e24d8fad81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A dict of all trigrams and their frequency\n",
        "all_trigrams = dict()\n",
        "\n",
        "# TODO: Add each unique trigram to the dictionary and set the value to how many times that trigram occurs in the text\n",
        "        \n",
        "len(all_trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2801c8a4-668d-4800-9b64-8094525e5306",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see what the twenty most common trigrams are\n",
        "sorted(all_trigrams.items(), key=lambda x: x[1], reverse=True)[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c76b65-79df-4c99-b5b9-6ce38cdc1ab4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## Fazer previsões usando n-gramas\n",
        "\n",
        "Agora que temos uma contagem de todos os trigramas, podemos fazer previsões procurando o trigrama mais comum que corresponda ao nosso input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e9146a-9fca-4dee-b59d-fb288397a96e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_trigram_model(text, number_results = 3):\n",
        "    input_tokens = pad(preprocess(text), 2)\n",
        "    \n",
        "    # Find the last 2 tokens in the input\n",
        "    last_two_tokens = input_tokens[-2:]\n",
        "    \n",
        "    # Search our list of all trigrams to find matching trigrams\n",
        "    matching_trigrams = []\n",
        "    for item in all_trigrams.items():\n",
        "        gram = item[0]\n",
        "        \n",
        "        # Check if the first and second item in the trigram are a match\n",
        "        if gram[0] == last_two_tokens[0] and gram[1] == last_two_tokens[1]:\n",
        "            matching_trigrams.append(item)\n",
        "    \n",
        "    # Now, sort the matching trigrams by popularity and return the first `number_results` results\n",
        "    sorted_matching_trigrams = sorted(matching_trigrams, key=lambda x: x[1], reverse=True)\n",
        "    top_matching_trigrams = sorted_matching_trigrams[:number_results]\n",
        "    \n",
        "    # Last, let's just get the predicted word (the last word of the trigram)\n",
        "    predictions = [trigram[0][2] for trigram in top_matching_trigrams]\n",
        "    return predictions\n",
        "\n",
        "predict_trigram_model(\"how is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09269f56-5273-47dc-9c1d-80b89e01bf46",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## Tornar o texto preditivo um app autônomo\n",
        "\n",
        "#### **Exercício 4**\n",
        "Crie uma UI usando a função `predict_trigram_model`. Consulte o corretor automático para obter ajuda.\n",
        "\n",
        "<details>\n",
        "  <summary>Mostrar resposta</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">autocompleter = gr.Interface(fn=predict_trigram_model, inputs=\"text\", outputs=\"text\", live=True)\n",
        "</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64018012-2f79-42bc-9fda-dd4c12332b9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "gr.close_all()\n",
        "\n",
        "autocompleter = None\n",
        "\n",
        "# TODO: Create a UI using Gradio for predictive text\n",
        "\n",
        "autocompleter.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bb2d2e-42dd-4933-86d0-3915715064bf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "# **Melhorar N-Gramas com Backoff**\n",
        "Nosso aplicativo tem um problema importante. Se a frase que inserimos termine com duas palavras que não correspondem a *qualquer* trigrama, então o nosso modelo não tem previsões a fazer.\n",
        "\n",
        "Uma solução comum para isso é usar um processo chamado **backoff**. Com Backoff, se o nosso modelo de trigrama não encontrar nenhum resultado, então tentaremos encontrar uma correspondência de *bigramas*. Se isso não der certo, vamos para *unigramas* (o que significa que usamos apenas as palavras mais frequentes). Isso significa que sempre obtemos um resultado, mesmo que não seja tão bom.\n",
        "\n",
        "Abaixo está um modelo que usa retrocessos e um n-gram de tamanho arbitrário para fazer previsões, agrupando tudo até agora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc969a71-88c0-480e-9e3c-c3cc5d6643f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_gram_models = dict()\n",
        "\n",
        "def create_ngram_model(n = 3):\n",
        "    padded_tokens = pad(tokens_filtered, n - 1)\n",
        "    grams = list(ngrams(sequence=padded_tokens, n=n))\n",
        "    \n",
        "    all_ngrams = dict()\n",
        "    for gram in grams:\n",
        "        if gram in all_ngrams:\n",
        "            all_ngrams[gram] += 1\n",
        "        else:\n",
        "            all_ngrams[gram] = 1\n",
        "    return all_ngrams\n",
        "\n",
        "\n",
        "def predict_ngram_model(text, n = 3, number_results = 3):\n",
        "    input_tokens = pad(preprocess(text), n - 1)\n",
        "    \n",
        "    while n > 0:\n",
        "        # Find the last n - 1 tokens in the input\n",
        "        last_tokens = tuple(input_tokens[-(n-1):])\n",
        "    \n",
        "        if not n in n_gram_models:\n",
        "            n_gram_models[n] = create_ngram_model(n)\n",
        "        \n",
        "        matching_ngrams = []\n",
        "        \n",
        "        for item in n_gram_models[n].items():\n",
        "            gram = item[0]\n",
        "            if gram[:-1] == last_tokens:\n",
        "                matching_ngrams.append(item)\n",
        "    \n",
        "        # Now, sort the matching n-grams by popularity and return the first `number_results` results\n",
        "        sorted_matching_ngrams = sorted(matching_ngrams, key=lambda x: x[1], reverse=True)\n",
        "        top_matching_ngrams = sorted_matching_ngrams[:number_results]\n",
        "        \n",
        "        # BACKOFF: If there are no results, drop n and try again\n",
        "        if len(top_matching_ngrams) == 0:\n",
        "            print(\"backing off to:\", n-1)\n",
        "            n = n - 1\n",
        "            continue\n",
        "    \n",
        "        # Last, let's just get the predicted word (the last word of the trigram)\n",
        "        predictions = [gram[0][-1] for gram in top_matching_ngrams]\n",
        "        return predictions\n",
        "    return []\n",
        "\n",
        "predict_ngram_model(\"why is\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666de95f-3710-4f00-ac5d-cc2d25bbfddf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Resumo**\n",
        "Neste tutorial, construímos uma ferramenta preditiva para uma língua com poucos recursos. Isso incluiu:\n",
        "- Criar uma lista de n-gramas do corpus\n",
        "- Usar n-gramas para prever a próxima palavra em uma sequência\n",
        "- Usar o backoff para utilizar n-gramas de tamanhos diferentes conforme necessário\n",
        "\n",
        "### **Desafios**\n",
        "1. Use a função de previsão repetidamente para prever várias sequenciais de palavras, a partir de alguma sequência. Por exemplo, a sequência \"O que\" deve ser seguida de \"ele faz\" ou \"ela faz\".\n",
        "1. Melhore o aplicativo de texto preditivo com botões que permitem adicionar uma sugestão ao texto que você está digitando.\n",
        "1. É um pouco chato mostrar sempre as previsões mais comuns. Adicione um elemento de aleatoriedade usando a biblioteca `random` do Python, para que a função de previsão não mostre sempre os mesmos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fa2b62-4378-40af-96ef-1519d5bf28a1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}