{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "143cab5b-e634-41f2-b851-48125bf8a501",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "# **Aprendizaje automático en Python**\n",
        "El aprendizaje automático (ML, por sus siglas en inglés) y la inteligencia artificial (IA) son aspectos increíblemente importantes del procesamiento del lenguaje en la actualidad. A menudo, los idiomas bien estudiados y documentados, como el inglés y el español, tienen enormes modelos de aprendizaje automático, entrenados sobre miles de millones y miles de millones de palabras. Sin embargo, también podemos usar el aprendizaje automático, a una escala menor, para idiomas poco estudiada y documentadas.\n",
        "\n",
        "Esta lección te dará una breve visión general del uso de un framework de aprendizaje automático en Python, pero no entrará en una gran profundidad de la discusión. Para aprender más sobre cómo usar el aprendizaje automático para el procesamiento de lenguas, recomendamos el libro [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) de Jurafsky y Martin.\n",
        "\n",
        "## **Análisis de Sentimento**\n",
        "La tarea que resolveremos con un modelo ML es el **análisis de sentimientos**. El análisis de sentimiento tiene como objetivo predecir si una porción de texto expresa un sentimiento positivo, negativo o neutral sobre el tema. Utilizaremos un conjunto de tweets de [here](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset?resource=download). Por ejemplo, el siguiente tweet se clasifica como positivo:\n",
        "\n",
        "> Al diablo con las críticas, Wolverine me pareció increíble. Pero no hay suficiente Dominic Monaghan para mi gusto. Pero no hay suficiente Dominic Monaghan para mi gusto.\n",
        "\n",
        "Pero este tweet es negativo:\n",
        "\n",
        "> ¡¡¡ESTE twitter me está volviendo loco...NO ME DEJA DESCARGAR UNA FOTO DE PERFIL!!! ¡¡...supongo que seguiré intentándolo!!\n",
        "\n",
        "## **Cargando datos**\n",
        "Primero, necesitamos cargar nuestro conjunto de datos y prepararlo para usarlo en un modelo. Dado que los datos están en un formato csv, debemos usar el módulo `csv` para ayudarnos a analizarlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "657e4652-aeee-47a0-87d4-1ba1bc449c1a",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "all_tweets = []\n",
        "all_sentiments = []\n",
        "\n",
        "with open('./Tweets.csv') as tweets_file:\n",
        "    # Create a csv parser\n",
        "    csv_reader = csv.reader(tweets_file)\n",
        "    \n",
        "    # Skip the first row, the headers\n",
        "    next(csv_reader, None)\n",
        "    \n",
        "    for row in csv_reader:\n",
        "        # The second column has the tweet text\n",
        "        all_tweets.append(row[1])\n",
        "        \n",
        "        # The fourth column has the sentiment label\n",
        "        all_sentiments.append(row[3])\n",
        "        \n",
        "print(all_tweets[:10])\n",
        "print(all_sentiments[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a15e90-b6c0-4254-9276-d2ae81b2e184",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "También reemplazaremos cada sentimiento por una etiqueta: 0 para neutral, 1 para positivo y 2 para negativo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1e12db2-5f14-428f-9255-713b9ba0e77e",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "all_sentiments_encoded = []\n",
        "for sentiment in all_sentiments:\n",
        "    if sentiment == 'neutral':\n",
        "        all_sentiments_encoded.append(0)\n",
        "    elif sentiment == 'positive':\n",
        "        all_sentiments_encoded.append(1)\n",
        "    elif sentiment == 'negative':\n",
        "        all_sentiments_encoded.append(2)\n",
        "    else:\n",
        "        print(\"Unexpected label found\")\n",
        "        break\n",
        "        \n",
        "print(all_sentiments_encoded[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54c71a25-a754-45c4-8b05-ee1adc3e7c0a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### División de entrenamiento/pruebas\n",
        "En el aprendizaje de máquinas, es usual dividir los datos en dos conjuntos: uno para entrenar el modelo y otro para probar el rendimiento posterior del modelo. Esto ayuda a evaluar el modelo de forma justa y a evitar el *sobreajustamiento*, a saber, que el modelo sólo funcione bien con los datos de entrenamiento.\n",
        "\n",
        "Utilizaremos el método `train_test_split` del paquete `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41d8f53e-f9bf-42b6-97da-e045fdca3d41",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(all_tweets, all_sentiments_encoded, test_size=0.3)\n",
        "\n",
        "print(len(train_sentences), \"training sentences\")\n",
        "print(len(test_sentences), \"testing sentences\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "049b9143-f60a-44e1-9c67-01a2d20b569d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Creando un Modelo**\n",
        "Ahora estamos listos para crear nuestro modelo. Utilizaremos [Keras](https://keras.io), uno de los frameworks más populares para el aprendizaje automático. Keras ofrece la configuración más sencilla pero la menos personalizable, por lo que es una buena opción para esta lección.\n",
        "\n",
        "### Vectorización\n",
        "Primero, nuestro modelo convertirá cada frase en un vector de valores binarios, donde cada posición representa la ocurrencia de una palabra. Por ejemplo, si el vector para una oración comienza con `[1, 0, ...]` y las palabras son `[malo, bueno, .. ]`, entonces el vector indica que la palabra `mal` ocurre en el tweet, pero la palabra `bueno` no.\n",
        "\n",
        "Para esto, usamos la capa keras `TextVectorization`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ebf8a26-c856-4589-81e2-95ca73d46df6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "text_vectorizer = keras.layers.TextVectorization(output_mode='multi_hot', # Create a vector in the style we described\n",
        "                                                 max_tokens=2500)         # Use only the 2500 most common words\n",
        "\n",
        "# Train the vectorizer using the training dataset\n",
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b77037b-d7bb-439d-ae2e-35450c98d405",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "Ahora podemos ver las 100 palabras más comunes en el conjunto de datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae30c52f-8a03-42a4-a6b0-2b73aeb7df5d",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(text_vectorizer.get_vocabulary()[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c61d32-5e0c-40fb-9687-4910de2114fa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "También podemos usar el vectorizador para codificar una frase y ver cómo se ve el resultado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b24e760c-6cc8-4062-8f2f-1c347901add3",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "text_vectorizer(\"I went to the store\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cdfbb95-2857-4e93-9427-01ab48ee1deb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Capas ocultas\n",
        "<div>\n",
        "<img src=\"../assets/mlp.png\" width=\"500\" style=\" display: block; margin-left: auto; margin-right: auto;\"/>\n",
        "</div>\n",
        "\n",
        "Una de las técnicas clave utilizadas en el aprendizaje automático es la incorporación de **capas ocultas**. En cada capa oculta, una función se aplica a las entradas con variables de peso que modifican la salida. El modelo ajusta las variables de peso durante el entrenamiento hasta que se prevea la salida correcta.\n",
        "\n",
        "Utilizar más capas ocultas permite que el modelo aprenda patrones más complicados. En este caso, los pesos determinarán cuánto contribuye cada palabra a la predicción final. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9a5b62b-32cb-4f50-bee9-d57d3d0a8ca2",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# The parameter indicates how many nodes will be in each layer\n",
        "hidden_layer1 = keras.layers.Dense(100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9fb6583-8080-447e-a261-262f70518126",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Poniéndolo todo junto \n",
        "Ahora, podemos reunir todos los componentes de nuestro modelo.\n",
        "\n",
        "Cada modelo debe usar una **función de pérdida**, que define cuánto error hay. El modelo intentará minimizar la función de pérdida y así hacer mejores predicciones. En este caso, utilizamos **pérdidas de entropía cruzada ('crossentropy loss')**, que calcula cuánto error hubo en una predicción entre etiquetas categóricas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f23b0185-17f1-466d-8a99-fc8079a59fbd",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "# Our inputs will be strings\n",
        "input_layer = keras.Input(shape=(1,), dtype='string')\n",
        "model.add(input_layer)\n",
        "\n",
        "# Add the vectorization layer\n",
        "model.add(text_vectorizer)\n",
        "\n",
        "# Add the hidden layers\n",
        "model.add(hidden_layer1)\n",
        "\n",
        "# Add the output layer\n",
        "# Since we have 3 possible output classes, the layer should have three nodes\n",
        "output_layer = keras.layers.Dense(3, activation='softmax')\n",
        "model.add(output_layer)\n",
        "\n",
        "# Compile the model\n",
        "# We use `categorical_crossentropy` for tasks that have multiple categorical outputs\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ead368fc-7150-45e2-9cf8-e48aa7415e93",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Entrenamiento\n",
        "Ahora que hemos construido un modelo, el siguiente paso es entrenarlo. Esto puede llevar algún tiempo, ya que el entrenamiento implica muchas operaciones matriciales de gran tamaño."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd7e601d-1038-4536-b538-7bb06522a754",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "history = model.fit(train_sentences, train_labels, epochs=7, verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "78a3e467-56fa-496c-9621-e7c833bd1d00",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "Podemos ver que a medida que nuestra pérdida disminuyó, la precisión de la formación aumentó. \n",
        "\n",
        "### Evaluación\n",
        "Ahora, vamos a evaluar el modelo con nuestros datos de prueba."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de4577d-6bba-4419-ad5a-acc357054adb",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.evaluate(test_sentences, test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60f06f60-497a-49ad-982d-c90b377f8494",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "Nuestra exactitud de la prueba fue algo menor que nuestra precisión en el entrenamiento, pero todavía mucho mejor que adivinar al azar. Crear modelos en los que el rendimiento de la prueba no sea significativamente peor que el rendimiento del entrenamiento es un objetivo clave en el aprendizaje automático.\n",
        "\n",
        "Por último, veamos nuestro modelo en acción. Podemos utilizar nuestro modelo para predecir el sentimiento de algún Tweet que inventemos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45cf120d-3be5-4c44-9559-6035762adcf6",
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict_sentiment(tweet):\n",
        "    # Our predictions will be a 3-element vector, where each element is the probability of a given sentiment class\n",
        "    predictions = model.predict([tweet])[0]\n",
        "    \n",
        "    # Take the argmax to find the most likely sentiment\n",
        "    predicted_sentiment_index = np.argmax(predictions)\n",
        "    \n",
        "    # Turn the sentiment index into a label\n",
        "    all_sentiments = ['neutral', 'positive', 'negative']\n",
        "    predicted_sentiment = all_sentiments[predicted_sentiment_index]\n",
        "    \n",
        "    return predicted_sentiment\n",
        "    \n",
        "\n",
        "print(predict_sentiment(\"I loved the new Guardians of the Galaxy movie. It was so well-made and touching!\"))\n",
        "print(predict_sentiment(\"I hated that movie. Gunn is a talentless hack\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e230e695-23fb-4e5f-81fb-20df70cff511",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Ejercicio de desafío 1**\n",
        "Intente modificar el modelo utilizado aquí para mejorar el rendimiento. Experimenta con el uso de un vocabulario más grande en el `TextVectorizer`, usando un número diferente de capas ocultas, o capas ocultas con un número diferente de nodos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43d8fc61-ceed-41e2-802f-342991bac4bb",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build and train a modified model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cdd97a9-1548-453f-8255-d5c8062f48db",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Ejercicio de desafío 2**\n",
        "Descargar [este conjunto de datos](https://www.kaggle.com/datasets/azimulh/tweets-data-for-authorship-attribution-modelling). Crea y entrena un modelo para predecir el autor de un tweet. Se trata de un problema similar al análisis de sentimientos, excepto que tenemos más de 3 etiquetas posibles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca4bf80-d51d-4308-a7a7-56234a9d67e3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Build and train a model for authorship prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d916a872-a3c2-4235-b95d-039b2eb1c6bd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Conclusión**\n",
        "En esta lección, hemos aprendido cómo crear, entrenar y evaluar un modelo de aprendizaje automático en Python para tareas de procesamiento del lenguaje. \n",
        "- Crear representaciones vectoriales de texto usando `TextVectorizer`\n",
        "- Construir un modelo con capas ocultas\n",
        "- Entrenar un modelo con un conjunto de datos de entrenamiento\n",
        "- Evaluar un modelo con un conjunto de datos de prueba\n",
        "\n",
        "El aprendizaje automático puede ser una herramienta poderosa para las aplicaciones del lenguaje, y puede aplicarse a una amplia gama de tareas. Independientemente de la tarea, las técnicas básicas mostradas aquí se utilizarán una y otra vez. \n",
        "\n",
        "En este punto, has completado todas las habilidades necesarias para comenzar a construir una tecnología de lenguaje útil. A continuación, echa un vistazo a los proyectos!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4742962-5e08-4bac-a4b4-9b9f45492d35",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}