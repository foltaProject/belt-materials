{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66bf47f6-8bda-4d52-9180-9ddcbd44ccc2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "# **2. Predicción de textos, Parte I: N-Grams**\n",
        "\n",
        "<div>\n",
        "<img src=\"assets/autocomplete.png\" width=\"500\" style=\" display: block; margin-left: auto; margin-right: auto;\"/>\n",
        "</div>\n",
        "\n",
        "En esta sección, construiremos un sistema de **predicción de textos** el cual sugiere la siguiente palabra mientras un usuario está escribiendo. Los predictores de texto se utilizan en teclados de teléfonos móviles, aplicaciones de búsqueda, sistemas de redacción de correo electrónico impulsados por IA, y más.\n",
        "\n",
        "El objetivo principal de un sistema de predicción de textos es que, *dada alguna secuencia de palabras, este pueda predecir la siguiente palabra según probabilidades*.\n",
        "\n",
        "#### ¿Cómo lo hacemos nosotros?\n",
        "En Procesamiento del Lenguaje Natural (PNL), esto se puede resolver con un **modelo del lenguaje**. Un modelo de lenguaje aprende la distribución de palabras en el texto. Construiremos un modelo de lenguaje sencillo que aprenda cadenas comunes de dos o tres palabras.\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a6f4658-fc08-4102-9060-29bca34ce78d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Preparación de datos**\n",
        "### Cargar el corpus\n",
        "Para esta aplicación, usaremos un texto en inglés tomado del corpus COCA. \n",
        "\n",
        "<div class=\"alert alert-block alert-warning\">\n",
        "    Como antes, siéntete libre de usar (un texto) de tu propia lengua en su lugar.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16a229d2-5414-4547-bcfd-32f23c822fe1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load our data\n",
        "import util\n",
        "\n",
        "# REPLACE WITH YOUR CORPUS DIRECTORY\n",
        "corpus = util.load_raw_text(corpus_directory=\"../corpora/eng\")\n",
        "corpus[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c09635b7-7801-497b-8bdd-5e8929a26c21",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Preprocesamiento y Tokenización\n",
        "Al igual que el corrector ortográfico, el siguiente paso es tokenizar el texto en palabras individuales. La única diferencia es que mantenemos la puntuación.\n",
        "\n",
        "#### **Ejercicio 1**\n",
        "Use la expresión regular proporcionada para tokenizar el texto, y devolver el resultado.\n",
        "\n",
        "<details>\n",
        "  <summary>Show answer</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">return re.findall(word_or_punctuation_regex, text)</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62e5d13b-434c-4a64-b53c-2ce55435306e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "word_or_punctuation_regex = r\"[\\w|\\']+|[\\.|\\,|\\?|\\!]\"\n",
        "\n",
        "def preprocess(text):\n",
        "    text = util.strip_accents(text)\n",
        "    text = text.lower()\n",
        "\n",
        "    # TODO: Use the provided regex to tokenize the text, and return the result\n",
        "\n",
        "tokens_filtered = preprocess(corpus)\n",
        "\n",
        "print(len(tokens_filtered), \"total tokens\")\n",
        "print(tokens_filtered[:200])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6801ad-251b-466f-9ea2-e50fa58b22b2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## **Modelado de N-Gram(as)**\n",
        "Ahora, vamos a construir nuestro primer modelo. Para esto usaremos *n-gram(as)*.\n",
        "\n",
        "Un propio n-grama es simplemente una secuencia de palabras *n* que aparece en nuestro texto. Por ejemplo, considera la frase:\n",
        "\n",
        "Cuál es tu Nombre? \n(n.t. con el fin de no modificar los ejemplos que siguen, se mantiene la convención ortográfica del inglés de usar solo un signo de cierre).\n",
        "\n",
        "Si obtuviéramos la lista de todos los *bigramas* (2 palabras cada una) en la oración, tendríamos:\n",
        "\n",
        "- *Cuál es*\n",
        "- *es tu*\n",
        "- *tu nombre*\n",
        "- *Nombre ?*\n",
        "\n",
        "Si obtuviéramos la lista de todos los *trigramas* (3 palabras cada una) en la oración, tendríamos:\n",
        "\n",
        "- *Cuál es tu*\n",
        "- *es tu nombre*\n",
        "- *tu nombre* ?*\n",
        "\n",
        "Y así pues, y así sucesivamente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49609b51-0373-45d2-bf28-d25a4ad8cb79",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Usando N-gramas como un modelo de lenguaje (natural)\n",
        "\n",
        "Podemos utilizar fácilmente la idea de los propios n-gramas para construir un sencillo modelo de lenguaje natural. Por ejemplo, si estamos intentando predecir la siguiente palabra, usando trigrams, dada la entrada:\n",
        "\n",
        "> Cuál es tu* ...\n",
        "\n",
        "Podemos ver todos los trigrams que empiezan con *es tu*, y elegir el más común. Usemos nuestro texto tokenizado para obtener una lista de todos los n-gramas que aparecen en el texto. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d3d04e-2ff1-45ac-9e76-8d97f69243ba",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Rellenando frases\n",
        "Ahora mismo, podemos tener n-gramas en los que se cruzan los límites de las frases, tales como \"titulares . tú\". Esto no es muy útil, así que una técnica común es **rellenar** cada oración con tokens representando el inicio de la oración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75ecaa5-650e-4a11-8e1a-eafca5a9681a",
      "metadata": {},
      "outputs": [],
      "source": [
        "def pad(text: list, num_padding: int):\n",
        "    \n",
        "    padded_text = []\n",
        "    \n",
        "    # Add initial padding to the first sentence\n",
        "    for _ in range(num_padding):\n",
        "        padded_text.append(\"<s>\")\n",
        "    \n",
        "    for word in text:\n",
        "        padded_text.append(word)\n",
        "\n",
        "        # Every time we see an end punctuation mark, add <s> tokens after it\n",
        "        # REPLACE IF YOUR LANGUAGE USES DIFFERENT END PUNCTUATION\n",
        "        if word in [\".\", \"?\", \"!\"]:\n",
        "            for _ in range(num_padding):\n",
        "                padded_text.append(\"<s>\")\n",
        "        \n",
        "        \n",
        "    return padded_text\n",
        "\n",
        "print(pad(tokens_filtered, 2)[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "add6e534-f8de-4b0d-bd82-f0637a8bf8c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "### Crea una lista de n-gramas\n",
        "El siguiente código utiliza la biblioteca **NLTK** para crear una lista de trigramas.\n",
        "\n",
        "#### **Ejercicio 2**\n",
        "¿Qué cambiaríamos para usar bigramas en su lugar?\n",
        "\n",
        "<details>\n",
        "  <summary>Show answer</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">padded_tokens = pad(tokens_filtered, 1)\n",
        "trigrams = list(ngrams(sequence=padded_tokens, n=2))</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8a031ba-3575-4080-917b-f38471fcb545",
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.util import ngrams\n",
        "\n",
        "# Now, we can actually create the list of n-grams using the NLTK library\n",
        "padded_tokens = pad(tokens_filtered, 2)\n",
        "trigrams = list(ngrams(sequence=padded_tokens, n=3))\n",
        "trigrams[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7450ed6-be5d-4565-a30a-e0d4703ec185",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "Ahora que tenemos una lista de triegramas, podemos contar la frecuencia de cada trigrama diferente.\n",
        "\n",
        "#### **Ejercicio 3**\n",
        "Utilizando la lista de trigramas, llena el diccionario `all_trigrams` de tal manera que cada llave es un trigrama único y el valor es cuántas veces dicho trigrama ocurre en el texto.\n",
        "\n",
        "<details>\n",
        "  <summary>Show answer</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">for gram in trigrams:\n",
        "    if gram in all_trigrams:\n",
        "        all_trigrams[gram] += 1\n",
        "    else:\n",
        "        all_trigrams[gram] = 1</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cccbdd25-9dca-42d1-ba39-60e24d8fad81",
      "metadata": {},
      "outputs": [],
      "source": [
        "# A dict of all trigrams and their frequency\n",
        "all_trigrams = dict()\n",
        "\n",
        "# TODO: Add each unique trigram to the dictionary and set the value to how many times that trigram occurs in the text\n",
        "        \n",
        "len(all_trigrams)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2801c8a4-668d-4800-9b64-8094525e5306",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see what the twenty most common trigrams are\n",
        "sorted(all_trigrams.items(), key=lambda x: x[1], reverse=True)[:30]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30c76b65-79df-4c99-b5b9-6ce38cdc1ab4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## Haciendo predicciones usando n-gramas\n",
        "\n",
        "Ahora que tenemos un recuento de todos los trigramas, podemos hacer predicciones buscando el trigrama más común que coincida con nuestro input. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39e9146a-9fca-4dee-b59d-fb288397a96e",
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_trigram_model(text, number_results = 3):\n",
        "    input_tokens = pad(preprocess(text), 2)\n",
        "    \n",
        "    # Find the last 2 tokens in the input\n",
        "    last_two_tokens = input_tokens[-2:]\n",
        "    \n",
        "    # Search our list of all trigrams to find matching trigrams\n",
        "    matching_trigrams = []\n",
        "    for item in all_trigrams.items():\n",
        "        gram = item[0]\n",
        "        \n",
        "        # Check if the first and second item in the trigram are a match\n",
        "        if gram[0] == last_two_tokens[0] and gram[1] == last_two_tokens[1]:\n",
        "            matching_trigrams.append(item)\n",
        "    \n",
        "    # Now, sort the matching trigrams by popularity and return the first `number_results` results\n",
        "    sorted_matching_trigrams = sorted(matching_trigrams, key=lambda x: x[1], reverse=True)\n",
        "    top_matching_trigrams = sorted_matching_trigrams[:number_results]\n",
        "    \n",
        "    # Last, let's just get the predicted word (the last word of the trigram)\n",
        "    predictions = [trigram[0][2] for trigram in top_matching_trigrams]\n",
        "    return predictions\n",
        "\n",
        "predict_trigram_model(\"how is\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09269f56-5273-47dc-9c1d-80b89e01bf46",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "## Hacer una aplicación independiente de predicción de textos\n",
        "\n",
        "#### **Ejercicio 4**\n",
        "Crea una interfaz de usuario usando la función `predict_trigram_model`. Consulte el ejercicio del corrector ortográfico para obtener ayuda.\n",
        "\n",
        "<details>\n",
        "  <summary>Show answer</summary>\n",
        "      <pre style=\"background-color: honeydew; padding: 10px; border-radius: 5px;\"><code style=\"background: none;\">autocompleter = gr.Interface(fn=predict_trigram_model, inputs=\"text\", outputs=\"text\", live=True)\n",
        "</code></pre>\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64018012-2f79-42bc-9fda-dd4c12332b9b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "autocompleter = None\n",
        "\n",
        "# TODO: Create a UI using Gradio for predictive text\n",
        "\n",
        "autocompleter.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5bb2d2e-42dd-4933-86d0-3915715064bf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "# **Mejorando N-Gramas con Backoff**\n",
        "Nuestra aplicación tiene un problema importante. Si la frase que ingresamos termina con dos palabras que no coinciden con *ninguna* trigrama, entonces nuestro modelo no tiene predicciones que hacer.\n",
        "\n",
        "Una solución común para esto es utilizar un proceso llamado **backoff**. Por medio del 'backoff', si nuestro modelo de trigramas no encuentra ningún resultado, intentamos encontrar *bigramas* coincidentes en su lugar. Si eso falla, vamos a *unigramas* (lo que significa que sólo usamos las palabras más frecuentes). Esto significa que siempre conseguimos un resultado, aunque no sea tan bueno.\n",
        "\n",
        "A continuación se muestra un modelo que utiliza backoff y un n-grama de tamaño arbitrario para hacer predicciones, juntando todo lo visto hasta ahora."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc969a71-88c0-480e-9e3c-c3cc5d6643f7",
      "metadata": {},
      "outputs": [],
      "source": [
        "n_gram_models = dict()\n",
        "\n",
        "def create_ngram_model(n = 3):\n",
        "    padded_tokens = pad(tokens_filtered, n - 1)\n",
        "    grams = list(ngrams(sequence=padded_tokens, n=n))\n",
        "    \n",
        "    all_ngrams = dict()\n",
        "    for gram in grams:\n",
        "        if gram in all_ngrams:\n",
        "            all_ngrams[gram] += 1\n",
        "        else:\n",
        "            all_ngrams[gram] = 1\n",
        "    return all_ngrams\n",
        "\n",
        "\n",
        "def predict_ngram_model(text, n = 3, number_results = 3):\n",
        "    input_tokens = pad(preprocess(text), n - 1)\n",
        "    \n",
        "    while n > 0:\n",
        "        # Find the last n - 1 tokens in the input\n",
        "        last_tokens = tuple(input_tokens[-(n-1):])\n",
        "    \n",
        "        if not n in n_gram_models:\n",
        "            n_gram_models[n] = create_ngram_model(n)\n",
        "        \n",
        "        matching_ngrams = []\n",
        "        \n",
        "        for item in n_gram_models[n].items():\n",
        "            gram = item[0]\n",
        "            if gram[:-1] == last_tokens:\n",
        "                matching_ngrams.append(item)\n",
        "    \n",
        "        # Now, sort the matching n-grams by popularity and return the first `number_results` results\n",
        "        sorted_matching_ngrams = sorted(matching_ngrams, key=lambda x: x[1], reverse=True)\n",
        "        top_matching_ngrams = sorted_matching_ngrams[:number_results]\n",
        "        \n",
        "        # BACKOFF: If there are no results, drop n and try again\n",
        "        if len(top_matching_ngrams) == 0:\n",
        "            print(\"backing off to:\", n-1)\n",
        "            n = n - 1\n",
        "            continue\n",
        "    \n",
        "        # Last, let's just get the predicted word (the last word of the trigram)\n",
        "        predictions = [gram[0][-1] for gram in top_matching_ngrams]\n",
        "        return predictions\n",
        "    return []\n",
        "\n",
        "predict_ngram_model(\"why is\", 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "666de95f-3710-4f00-ac5d-cc2d25bbfddf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "tags": []
      },
      "source": [
        "**Resumen**\n",
        "En este tutorial construimos una herramienta de predicción de texto para un lenguaje poco estudiado/documentado. Esto incluye:\n",
        "- Crear una lista de n-gramas a partir de un corpus\n",
        "- Utilizar n-gramas para predecir la siguiente palabra en una cadena de texto\n",
        "- Usar 'backoff' para utilizar diferentes tamaños de n-gramas según sea necesario\n",
        "\n",
        "### **Desafíos**\n",
        "1. Utiliza la función de predicción repetidamente para predecir múltiples palabras secuenciales dada alguna cadena de texto. Por ejemplo, la cadena \"Why is\" podría ir seguida de \"he doing\" o \"she haciendo\".\n",
        "2. Mejora la aplicación de texto predictivo con botones que te permiten añadir una sugerencia al texto que estás escribiendo.\n",
        "3. Es un poco aburrido mostrar siempre las mejores predicciones. Añade un elemento de aleatoriedad utilizando la biblioteca `random` de Python, para que la función de predicción no muestre siempre los mismos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fa2b62-4378-40af-96ef-1519d5bf28a1",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}