{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "143cab5b-e634-41f2-b851-48125bf8a501",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "# **Machine Learning in Python**\n",
    "Machine learning (ML) and artificial intelligence (AI) are incredibly important aspects of language processing nowadays. Often, high-resource languages such as English and Spanish have enormous ML models, trained on billions and billions of words. However, we can also use machine learning, at a smaller scale, for low-resource languages.\n",
    "\n",
    "This lesson will give a brief overview of using a machine learning framework in Python, but it won't go into a great amount of depth. To learn more about using machine learning for language processing, we recommend the book [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) by Jurafsky and Martin.\n",
    "\n",
    "## **Sentiment Analysis**\n",
    "The task we will be solving with an ML model is **sentiment analysis**. Sentiment analysis aims to predict whether a chunk of text expresses a positive, negative, or neutral sentiment about the topic. We will use a dataset of Tweets from [here](https://www.kaggle.com/datasets/yasserh/twitter-tweets-sentiment-dataset?resource=download). For instance, the following tweet is classified as positive:\n",
    "\n",
    "> Screw the reviews, I thought Wolverine was awesome. But not enough Dominic Monaghan for my liking.\n",
    "\n",
    "But this tweet is negative:\n",
    "\n",
    "> THIS twitter is driving me nuts...WONT LET ME DOWNLOAD A PROFILE PIC!! ...guess i`ll keep trying!!\n",
    "\n",
    "## **Loading Data**\n",
    "First, we need to load our dataset and prepare it to use in a model. Since the data is in a csv format, we must use the `csv` module to help parse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657e4652-aeee-47a0-87d4-1ba1bc449c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "all_tweets = []\n",
    "all_sentiments = []\n",
    "\n",
    "with open('./Tweets.csv') as tweets_file:\n",
    "    # Create a csv parser\n",
    "    csv_reader = csv.reader(tweets_file)\n",
    "    \n",
    "    # Skip the first row, the headers\n",
    "    next(csv_reader, None)\n",
    "    \n",
    "    for row in csv_reader:\n",
    "        # The second column has the tweet text\n",
    "        all_tweets.append(row[1])\n",
    "        \n",
    "        # The fourth column has the sentiment label\n",
    "        all_sentiments.append(row[3])\n",
    "        \n",
    "print(all_tweets[:10])\n",
    "print(all_sentiments[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a15e90-b6c0-4254-9276-d2ae81b2e184",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "We will also replace each sentiment with a label: 0 for neutral, 1 for positive, and 2 for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e12db2-5f14-428f-9255-713b9ba0e77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_sentiments_encoded = []\n",
    "for sentiment in all_sentiments:\n",
    "    if sentiment == 'neutral':\n",
    "        all_sentiments_encoded.append(0)\n",
    "    elif sentiment == 'positive':\n",
    "        all_sentiments_encoded.append(1)\n",
    "    elif sentiment == 'negative':\n",
    "        all_sentiments_encoded.append(2)\n",
    "    else:\n",
    "        print(\"Unexpected label found\")\n",
    "        break\n",
    "        \n",
    "print(all_sentiments_encoded[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c71a25-a754-45c4-8b05-ee1adc3e7c0a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Training/testing split\n",
    "In machine learning, it is traditional to divide data into two datasets: one for training the model, and one to test the model's performance afterward. This helps to evaluate the model fairly and to avoid *overfitting*, where the model only works well on the trianing data.\n",
    "\n",
    "We will use the `train_test_split` method from the `sklearn` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d8f53e-f9bf-42b6-97da-e045fdca3d41",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, test_sentences, train_labels, test_labels = train_test_split(all_tweets, all_sentiments_encoded, test_size=0.3)\n",
    "\n",
    "print(len(train_sentences), \"training sentences\")\n",
    "print(len(test_sentences), \"testing sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049b9143-f60a-44e1-9c67-01a2d20b569d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Creating a Model**\n",
    "Now we are ready to create our model. We will use [Keras](https://keras.io), one of the popular frameworks for machine learning. Keras offers the easiest setup but the least customizability, making it a good choice for this lesson.\n",
    "\n",
    "### Vectorization\n",
    "First, our model will convert each sentence into a vector of binary values, where each position represents the occurrence of a word. For instance, if the vector for a sentence starts with `[1, 0, ...]` and the words are `[bad, good, ...]`, then the vector indicates that the word `bad` occurs in the tweet but the word `good ` does not.\n",
    "\n",
    "For this, we use the keras `TextVectorization` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebf8a26-c856-4589-81e2-95ca73d46df6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "text_vectorizer = keras.layers.TextVectorization(output_mode='multi_hot', # Create a vector in the style we described\n",
    "                                                 max_tokens=2500)         # Use only the 2500 most common words\n",
    "\n",
    "# Train the vectorizer using the training dataset\n",
    "text_vectorizer.adapt(train_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b77037b-d7bb-439d-ae2e-35450c98d405",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Now we can see the top 100 most common words in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30c52f-8a03-42a4-a6b0-2b73aeb7df5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(text_vectorizer.get_vocabulary()[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c61d32-5e0c-40fb-9687-4910de2114fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "We can also use the vectorizer to encode a sentence and see what the result looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24e760c-6cc8-4062-8f2f-1c347901add3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_vectorizer(\"I went to the store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdfbb95-2857-4e93-9427-01ab48ee1deb",
   "metadata": {
    "deletable": false,
    "editable": true,
    "tags": []
   },
   "source": [
    "### Hidden Layers\n",
    "<div>\n",
    "<img src=\"../../assets/mlp.png\" width=\"500\" style=\" display: block; margin-left: auto; margin-right: auto;\"/>\n",
    "</div>\n",
    "\n",
    "One of the key techniques used in machine learning is the incorporation of **hidden layers**. At each hidden layer, a function is applied to the inputs with weight variables that modify the output. The model adjusts the weight variables during training until the correct output is predicted.\n",
    "\n",
    "Using more hidden layers allows for a model that can learn more complicated patterns. In this case, the weights will determine how much each word contributes to the final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a5b62b-32cb-4f50-bee9-d57d3d0a8ca2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The parameter indicates how many nodes will be in each layer\n",
    "hidden_layer1 = keras.layers.Dense(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb6583-8080-447e-a261-262f70518126",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Putting it together\n",
    "Now, we can put together all the components of our model.\n",
    "\n",
    "Every model must use a **loss function**, which defines how much error there is. The model will attempt to minimize the loss function and thus make better predictions. In this case, we use **crossentropy loss**, which calculates how much error there was in a prediction between categorical labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b0185-17f1-466d-8a99-fc8079a59fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Our inputs will be strings\n",
    "input_layer = keras.Input(shape=(1,), dtype='string')\n",
    "model.add(input_layer)\n",
    "\n",
    "# Add the vectorization layer\n",
    "model.add(text_vectorizer)\n",
    "\n",
    "# Add the hidden layers\n",
    "model.add(hidden_layer1)\n",
    "\n",
    "# Add the output layer\n",
    "# Since we have 3 possible output classes, the layer should have three nodes\n",
    "output_layer = keras.layers.Dense(3, activation='softmax')\n",
    "model.add(output_layer)\n",
    "\n",
    "# Compile the model\n",
    "# We use `categorical_crossentropy` for tasks that have multiple categorical outputs\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead368fc-7150-45e2-9cf8-e48aa7415e93",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "### Training\n",
    "Now that we've built a model, the next step is training it. This may take some time, since training involves many large matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e601d-1038-4536-b538-7bb06522a754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_sentences, train_labels, epochs=7, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a3e467-56fa-496c-9621-e7c833bd1d00",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "We can see that as our loss decreased, training accuracy increased. \n",
    "\n",
    "### Evaluation\n",
    "Now, let's evaluate the model on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de4577d-6bba-4419-ad5a-acc357054adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_sentences, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f06f60-497a-49ad-982d-c90b377f8494",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "Our test accuracy was somewhat lower than our training accuracy, but still far better than guessing at random. Building models where the test performance isn't significantly worse than the training performance is a key goal in machine learning.\n",
    "\n",
    "Finally, let's see our model in action. We can use our model to predict the sentiment of some Tweet we make up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cf120d-3be5-4c44-9559-6035762adcf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def predict_sentiment(tweet):\n",
    "    # Our predictions will be a 3-element vector, where each element is the probability of a given sentiment class\n",
    "    predictions = model.predict([tweet])[0]\n",
    "    \n",
    "    # Take the argmax to find the most likely sentiment\n",
    "    predicted_sentiment_index = np.argmax(predictions)\n",
    "    \n",
    "    # Turn the sentiment index into a label\n",
    "    all_sentiments = ['neutral', 'positive', 'negative']\n",
    "    predicted_sentiment = all_sentiments[predicted_sentiment_index]\n",
    "    \n",
    "    return predicted_sentiment\n",
    "    \n",
    "\n",
    "print(predict_sentiment(\"I loved the new Guardians of the Galaxy movie. It was so well-made and touching!\"))\n",
    "print(predict_sentiment(\"I hated that movie. Gunn is a talentless hack\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230e695-23fb-4e5f-81fb-20df70cff511",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Challenge Exercise 1**\n",
    "Try modifying the model used here to improve performance. Experiment with using a larger vocabulary in the `TextVectorizer`, using a different number of hidden layers, or hidden layers with a different number of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8fc61-ceed-41e2-802f-342991bac4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and train a modified model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdd97a9-1548-453f-8255-d5c8062f48db",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Challenge Exercise 2**\n",
    "Download [this dataset](https://www.kaggle.com/datasets/azimulh/tweets-data-for-authorship-attribution-modelling). Create and train a model for predicting the author of a tweet. This is a similar problem to sentiment analysis, except we have more than 3 possible labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4bf80-d51d-4308-a7a7-56234a9d67e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build and train a model for authorship prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d916a872-a3c2-4235-b95d-039b2eb1c6bd",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "## **Conclusion**\n",
    "In this lesson, we learned what it looks like to create, train, and evaluate a machine learning model in Python for language processing tasks. \n",
    "- Creating vector representations of text using `TextVectorizer`\n",
    "- Building a model with hidden layers\n",
    "- Training a model on a training dataset\n",
    "- Evaluating a model with a testing dataset\n",
    "\n",
    "Machine learning can be a powerful tool for language applications, and it can be applied to a huge range of tasks. Regardless of the task, the basic techniques shown here will be used over and over. \n",
    "\n",
    "At this point, you've completed all of the skills necessary to begin building usable language technology. Next, take a look at the projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4742962-5e08-4bac-a4b4-9b9f45492d35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
